
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="..">
      
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.15">
    
    
      
        <title>Training Cookbook - WebThing</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.342714a4.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    <body dir="ltr">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#training-cookbook" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="WebThing" class="md-header__button md-logo" aria-label="WebThing" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            WebThing
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Training Cookbook
            
          </span>
        </div>
      </div>
    </div>
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href=".." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
  
  
    <li class="md-tabs__item md-tabs__item--active">
      <a href="./" class="md-tabs__link">
        
  
  
    
  
  Training Cookbook

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="WebThing" class="md-nav__button md-logo" aria-label="WebThing" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    WebThing
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Training Cookbook
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Training Cookbook
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#device-mesh-and-shardings" class="md-nav__link">
    <span class="md-ellipsis">
      Device Mesh and Shardings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Device Mesh and Shardings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#device-mesh" class="md-nav__link">
    <span class="md-ellipsis">
      Device Mesh
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#train-state-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Train State Initialization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Train State Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Initialization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer Initialization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-train-step-functional-transformations" class="md-nav__link">
    <span class="md-ellipsis">
      The Train Step (Functional Transformations)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Train Step (Functional Transformations)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      Model Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-and-optimizer-update" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient and Optimizer Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      The Training Loop
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Training Loop">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#efficiency-via-asynchronous-dispatch" class="md-nav__link">
    <span class="md-ellipsis">
      Efficiency via Asynchronous Dispatch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-mistakes" class="md-nav__link">
    <span class="md-ellipsis">
      Common Mistakes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Mistakes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#requesting-device-to-host-transfers" class="md-nav__link">
    <span class="md-ellipsis">
      Requesting device-to-host transfers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interrupting-the-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      Interrupting the accelerator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Data Loading
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#achieving-high-performance" class="md-nav__link">
    <span class="md-ellipsis">
      Achieving High Performance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Achieving High Performance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Data Parallel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensor-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Tensor Parallel
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#device-mesh-and-shardings" class="md-nav__link">
    <span class="md-ellipsis">
      Device Mesh and Shardings
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Device Mesh and Shardings">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#device-mesh" class="md-nav__link">
    <span class="md-ellipsis">
      Device Mesh
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#train-state-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Train State Initialization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Train State Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#parameter-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Parameter Initialization
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimizer-initialization" class="md-nav__link">
    <span class="md-ellipsis">
      Optimizer Initialization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-train-step-functional-transformations" class="md-nav__link">
    <span class="md-ellipsis">
      The Train Step (Functional Transformations)
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Train Step (Functional Transformations)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#model-forward-pass" class="md-nav__link">
    <span class="md-ellipsis">
      Model Forward Pass
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-and-optimizer-update" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient and Optimizer Update
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#the-training-loop" class="md-nav__link">
    <span class="md-ellipsis">
      The Training Loop
    </span>
  </a>
  
    <nav class="md-nav" aria-label="The Training Loop">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#efficiency-via-asynchronous-dispatch" class="md-nav__link">
    <span class="md-ellipsis">
      Efficiency via Asynchronous Dispatch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#common-mistakes" class="md-nav__link">
    <span class="md-ellipsis">
      Common Mistakes
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Common Mistakes">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#requesting-device-to-host-transfers" class="md-nav__link">
    <span class="md-ellipsis">
      Requesting device-to-host transfers
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interrupting-the-accelerator" class="md-nav__link">
    <span class="md-ellipsis">
      Interrupting the accelerator
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#data-loading" class="md-nav__link">
    <span class="md-ellipsis">
      Data Loading
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#achieving-high-performance" class="md-nav__link">
    <span class="md-ellipsis">
      Achieving High Performance
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Achieving High Performance">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#data-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Data Parallel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tensor-parallel" class="md-nav__link">
    <span class="md-ellipsis">
      Tensor Parallel
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="training-cookbook">Training Cookbook</h1>
<p>Traditionally, machine learning codebases rely on libraries to perform much of the bookkeeping and parameter wrangling necessary for training large, complex models. While convenient, these libraries can abstract the key functionality and core APIs offered in JAX. The purpose of this cookbook, therefore, is to demonstrate best practices (or "recipes") for writing simple yet high-performance machine learning training code directly in JAX. Following the patterns documented below will prepare your machine learning workloads to maximally leverage our compiler (XLA) for performance and tractability.</p>
<p>Most training scripts adhere roughly to the following structure:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_loop</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
  <span class="n">record_writer</span> <span class="o">=</span> <span class="n">RecordWriter</span><span class="p">()</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">mutable_array</span><span class="p">,</span> <span class="n">train_state</span><span class="p">)</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">get_dataset_on_device</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_steps</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</code></pre></div>
For each line of code above, we will explain the best practices and showcase the core technologies we have assembled to empower you to write simple, yet unbelievably performant code in JAX. The code above is a segment of a self-contained, <a href="https://github.com/ZacCranko/training-cookbook/blob/main/training_cookbook.py">completely functional companion script</a> in which we initialize a <a href="https://arxiv.org/abs/1706.03762">Vaswani et al. (2017)</a> Transformer decoder and define the training loss for next-token prediction in pure JAX. The code therein is suited to TPUs, CPUs, and GPUs, as well as single- and multi-host systems. For that reason, we use the terms <em>device</em> or <em>accelerator</em> to refer interchangeably to the hardware JAX is primarily performing arithmetic on—whether it be a TPU, GPU, or CPU—and <em>host system</em> to refer to operations performed exclusively using the host CPU.</p>
<p>In this guide, there are many aspects of the JAX APIs we will gloss over for the sake of expediency. These are available for you to peruse at your leisure in our API documentation. However, there is a central JAX concept that one must confront in detail for much of what follows to cohere.</p>
<h3 id="device-mesh-and-shardings">Device Mesh and Shardings</h3>
<p>JAX employs the <a href="https://en.wikipedia.org/wiki/Single_program,_multiple_data"><em>Single Program, Multiple Data</em> (SPMD)</a> model of parallelism. This means we write a single program that runs on multiple devices, using annotations to specify which part of the data each device is responsible for. The two primary concepts for this are the <code>Mesh</code> and the <code>PartitionSpec</code>.</p>
<h4 id="device-mesh">Device Mesh</h4>
<p>A <code>jax.sharding.Mesh</code> is an arrangement of all our accelerators into a NumPy <code>ndarray</code>, together with string labels for the axes of the device array. The reason for using an array is that this allows for a very convenient annotation for how arrays should be partitioned across devices. For this introduction, we will use the notation of an ordered dictionary<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup>, so that <code>{"x": 2, "y": 4}</code> refers to a device mesh of shape <code>(2, 4)</code> with labeled axes <code>"x"</code> and <code>"y"</code>. To shard an array <code>param</code>, we decorate it with a <code>PartitionSpec</code>, which is a tuple of <code>str | None</code> elements of the same length as the dimensions of the array. The <code>PartitionSpec</code> specifies which axes of our array are to be sharded over which axes of devices. A more thorough account of the notation of shardings and sharded computations is available in our <a href="https://docs.jax.dev/en/latest/sharded-computation.html">sharding tutorial</a>. Some common sharding strategies such as data parallel, fully sharded data parallel, and basic tensor parallelism will be covered in <a href="#achieving-high-performance">Achieving High Performance</a>.</p>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>Suppose we have a device mesh of <code>{"x": 2, "y": 4}</code> and an array <code>param</code> of shape <code>(32, 64, 64, 128)</code>. If we shard this array with <code>PartitionSpec(None, "x", "y", None, None)</code>, we end up with shards of size <code>(32, 32, 16, 128)</code> distributed across the devices. The <code>None</code> indicates that an axis should not be sharded. JAX implicitly broadcasts trailing axes, so an identical sharding can be achieved more concisely with <code>PartitionSpec(None, "x", "y")</code>. As a result, the shorthand for a fully replicated array (of any dimension) is <code>PartitionSpec()</code>.</p>
</div>
<div class="admonition example">
<p class="admonition-title">Example</p>
<p>More advanced mesh geometries are convenient when aligned with the communication hierarchy of our devices. Host-to-host communication is typically slower than accelerator-to-accelerator communication. Suppose we have two host machines, each with eight attached GPUs. One might arrange the devices into a mesh of <code>{"host": 2, "gpu": 8}</code>. Then we can shard a parameter as follows:
<div class="highlight"><pre><span></span><code><span class="n">param</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">192</span><span class="p">),</span> <span class="n">device</span><span class="o">=</span><span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;gpu&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">))</span>
</code></pre></div>
The whole of <code>param</code> will be replicated twice, but within each host, it will be spread across the eight locally attached GPUs, with each GPU storing a shard of shape <code>(32, 192)</code> in HBM. This is particularly useful for <a href="#fully-sharded-data-parallel-fsdp">FSDP sharding</a>.</p>
</div>
<h2 id="train-state-initialization">Train State Initialization</h2>
<p><div class="highlight"><pre><span></span><code><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">()</span>
  <span class="n">train_state</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">init_adam_state</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_state</span>
</code></pre></div>
Before we can get started, the first thing we need to do is set up the train state. The train state encapsulates (unsurprisingly) all the <em>stateful</em> aspects of the training process. This typically includes, at a minimum, the model parameters and the optimizer state. The way we have structured this function (though you may choose to do otherwise) is to:</p>
<ol>
<li>
<p>Create a series of nested dictionaries to house the model parameters, and then</p>
</li>
<li>
<p><code>jax.tree.map</code> over those parameters to produce a similar set of nested dictionaries to house the accompanying optimizer states. (More on this <a href="#optimizer-initialization">below</a>.)</p>
</li>
</ol>
<h3 id="parameter-initialization">Parameter Initialization</h3>
<p><div class="highlight"><pre><span></span><code><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">()</span>
<span class="hll">  <span class="n">train_state</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
</span>  <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">init_adam_state</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">train_state</span>
</code></pre></div>
To initialize our parameters, we build a series of nested dictionaries that correspond to the semantic sections of the neural network. If we were using a layer-based library such as PyTorch or Flax, these might correspond to neural network layers. For this example, we could, in fact, get by with a completely flattened dictionary, but the nested approach is convenient both for working with some of the APIs in JAX and for structuring our code.</p>
<p><div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">root_key</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">key</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">param_seed</span><span class="p">)</span>
  <span class="n">key</span> <span class="o">=</span> <span class="nb">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">fold_in</span><span class="p">,</span> <span class="n">root_key</span><span class="p">),</span> <span class="n">it</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
  <span class="n">zero_init</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>
  <span class="n">he_init</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">he_normal</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
  <span class="n">dtype</span> <span class="o">=</span> <span class="n">config</span><span class="o">.</span><span class="n">dtype</span>

  <span class="n">params</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
    <span class="n">pos_embed</span><span class="o">=</span><span class="n">zero_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">seq_length</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">pos_embed</span><span class="p">),</span>
    <span class="n">layers</span><span class="o">=</span><span class="n">dot_dict</span><span class="p">(),</span>
  <span class="p">)</span>
  <span class="n">params</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed</span><span class="p">)</span>
  <span class="n">params</span><span class="o">.</span><span class="n">linear_in</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">in_kernel</span><span class="p">),</span>
    <span class="n">bias</span><span class="o">=</span><span class="n">zero_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">in_bias</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="n">params</span><span class="o">.</span><span class="n">linear_out</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
    <span class="n">kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">out_kernel</span><span class="p">),</span>
  <span class="p">)</span>
  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">qkv_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">)</span>
    <span class="n">out_shape</span> <span class="o">=</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">head_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">)</span>
    <span class="n">params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span>
      <span class="n">attention</span><span class="o">=</span><span class="n">dot_dict</span><span class="p">(</span>
        <span class="n">qkv</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">qkv_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">att_qkv</span><span class="p">),</span>
        <span class="n">out</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="n">out_shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">att_out</span><span class="p">),</span>
      <span class="p">),</span>
      <span class="n">mlp</span><span class="o">=</span><span class="n">dot_dict</span><span class="p">(</span>
        <span class="n">in_kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_in</span><span class="p">),</span>
        <span class="n">out_kernel</span><span class="o">=</span><span class="n">he_init</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">key</span><span class="p">),</span> <span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">mlp_dim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">),</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">mlp_out</span><span class="p">),</span>
      <span class="p">),</span>
    <span class="p">)</span>
  <span class="k">return</span> <span class="n">params</span>
</code></pre></div>
Our <code>get_param_state</code> function makes use of the <code>constant</code> and <code>he_normal</code> factories provided in <a href="https://docs.jax.dev/en/latest/jax.nn.initializers.html"><code>jax.nn.initializers</code></a>. These factories return an <em>initializer</em>, which is a function conforming to the following protocol:
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">Initializer</span><span class="p">(</span><span class="n">Protocol</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="p">,</span> <span class="n">out_sharding</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
        <span class="o">...</span>
</code></pre></div>
The functional flavor of JAX requires explicit handling of all stochasticity, so we set up a little iterator that yields PRNG keys. Then, to build our parameters, we initialize them at their respective positions in the <code>params</code> nested dictionary, supplying the parameter shape, dtype, and sharding from the <code>Config</code> class.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>By specifying the shardings here, we initialize each shard of each parameter directly on the correct device in the device mesh where it needs to be, preventing the need for needless host-to-device transfers or, in the case of a model that does not fit in system memory, avoiding out-of-memory errors.</p>
</div>
<h3 id="optimizer-initialization">Optimizer Initialization</h3>
<p><div class="highlight"><pre><span></span><code><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">()</span>
  <span class="n">train_state</span><span class="o">.</span><span class="n">params</span> <span class="o">=</span> <span class="n">init_param_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="hll">  <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">init_adam_state</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
</span>  <span class="k">return</span> <span class="n">train_state</span>
</code></pre></div>
When it comes to setting up the optimizer state, things are a little less straightforward than when we built the model parameters. The <a href="https://arxiv.org/abs/1412.6980">Adam optimizer</a> requires that, for each parameter, we keep track of three optimization states: <code>mu</code>, <code>nu</code>, and <code>count</code>. The simplest of these is <code>count</code>, which stores the number of training steps we have performed. This is just a scalar used to de-bias the Adam updates. The <code>mu</code> and <code>nu</code> states will be arrays of the same shape, dtype, and sharding as the accompanying parameter <code>param</code>.<sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">init_adam_state</span><span class="p">(</span><span class="n">param</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">dot_dict</span><span class="p">:</span>
  <span class="n">adam_state</span> <span class="o">=</span> <span class="n">dot_dict</span><span class="p">(</span><span class="n">mu</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">nu</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">param</span><span class="p">),</span> <span class="n">count</span><span class="o">=</span><span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
  <span class="k">return</span> <span class="n">adam_state</span>
</code></pre></div>
When we use <code>jax.tree.map</code>, it iterates over the items in <code>train_state.params</code>. For each parameter, it creates a corresponding Adam state, resulting in a new nested dictionary that mirrors the structure of <code>train_state.params</code>. Each leaf in this new structure contains the optimizer state for the corresponding parameter.</p>
<h2 id="the-train-step-functional-transformations">The Train Step (Functional Transformations)</h2>
<p><div class="highlight"><pre><span></span><code><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
  <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model_apply</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;observed_ids&quot;</span><span class="p">])</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;target_ids&quot;</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

  <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">methodcaller</span><span class="p">(</span><span class="s2">&quot;get&quot;</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="p">)</span>
  <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
  <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>
The train step is where we calculate the gradient of the model with respect to the current parameters and use the gradient, together with the optimizer, to update the parameters. To do this in JAX, we define the forward pass of the model, then we leverage JAX's functional transformations to automatically generate the backward pass, which we use to calculate the gradients and perform the update.</p>
<h3 id="model-forward-pass">Model Forward Pass</h3>
<p><div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">model_apply</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">params</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">,</span> <span class="n">tokens</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">:</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="n">tokens</span><span class="p">]</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
  <span class="n">out</span> <span class="o">+=</span> <span class="n">params</span><span class="o">.</span><span class="n">pos_embed</span>
  <span class="k">del</span> <span class="n">tokens</span>

  <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_layers</span><span class="p">):</span>
    <span class="n">block</span> <span class="o">=</span> <span class="n">params</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="n">layer</span><span class="p">]</span>
    <span class="n">att_skip</span> <span class="o">=</span> <span class="n">out</span>  <span class="c1"># 1 billion dollars in venture capital funding please</span>
    <span class="n">qkv</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsd,3dkh-&gt;bs3kh&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">qkv</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_att</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">dot_product_attention</span><span class="p">(</span><span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:],</span> <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:],</span> <span class="n">qkv</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">:],</span> <span class="n">is_causal</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bskh,khd-&gt;bsd&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">attention</span><span class="o">.</span><span class="n">out</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">att_skip</span>
    <span class="n">out</span> <span class="o">*=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

    <span class="n">mlp_skip</span> <span class="o">=</span> <span class="n">out</span>  <span class="c1"># machine learning circa 1986</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsd,dh-&gt;bsh&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">in_kernel</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_hidden</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">gelu</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsh,hd-&gt;bsd&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">block</span><span class="o">.</span><span class="n">mlp</span><span class="o">.</span><span class="n">out_kernel</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">+=</span> <span class="n">mlp_skip</span>
    <span class="n">out</span> <span class="o">*=</span> <span class="n">jax</span><span class="o">.</span><span class="n">lax</span><span class="o">.</span><span class="n">rsqrt</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-6</span><span class="p">)</span>

  <span class="n">logits</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">einsum</span><span class="p">(</span><span class="s2">&quot;bsd,dl-&gt;bsl&quot;</span><span class="p">,</span> <span class="n">out</span><span class="p">,</span> <span class="n">params</span><span class="o">.</span><span class="n">linear_out</span><span class="o">.</span><span class="n">kernel</span><span class="p">,</span> <span class="n">out_sharding</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">act_seq</span><span class="p">)</span>
  <span class="k">return</span> <span class="n">logits</span>
</code></pre></div>
The model's forward pass is mostly unremarkable, aside from the <code>out_sharding</code> annotations we have supplied. These annotations declare what the result-sharding should be after the operation executes. The compiler uses these activation shardings, together with the parameter shardings we supplied when we <a href="#parameter-initialization">initialized the model</a>, to dynamically insert <a href="https://en.wikipedia.org/wiki/Collective_operation">communication collectives</a> that ferry parameters and activations alike between devices.</p>
<p>By choosing a good sharding strategy, we can achieve highly performant training (and inference) code. We will cover some standard strategies that serve most use cases in the section titled <a href="#achieving-high-performance">Achieving High Performance</a>. For a detailed discussion of the principles underpinning the design of sharding strategies, see <a href="https://jax-ml.github.io/scaling-book/">The Scaling Cookbook</a>.</p>
<h3 id="gradient-and-optimizer-update">Gradient and Optimizer Update</h3>
<p><div class="highlight"><pre><span></span><code><span class="nd">@jax</span><span class="o">.</span><span class="n">jit</span>
<span class="k">def</span><span class="w"> </span><span class="nf">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="nb">dict</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">dict</span><span class="p">:</span>
<span class="hll">  <span class="k">def</span><span class="w"> </span><span class="nf">loss_fn</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
</span><span class="hll">    <span class="n">logits</span> <span class="o">=</span> <span class="n">model_apply</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s2">&quot;observed_ids&quot;</span><span class="p">])</span>
</span><span class="hll">    <span class="n">labels</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">batch</span><span class="p">[</span><span class="s2">&quot;target_ids&quot;</span><span class="p">],</span> <span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">)</span>
</span><span class="hll">    <span class="k">return</span> <span class="o">-</span><span class="p">(</span><span class="n">labels</span> <span class="o">*</span> <span class="n">jax</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">log_softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</span>
  <span class="n">params</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">op</span><span class="o">.</span><span class="n">methodcaller</span><span class="p">(</span><span class="s2">&quot;get&quot;</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
  <span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
  <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="p">)</span>
  <span class="n">metrics</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;train_loss&quot;</span><span class="p">:</span> <span class="n">loss</span><span class="p">}</span>
  <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>
In order to calculate the gradient, we define the training loss. This is a function of the parameters that returns a scalar which summarizes how well our model, with the current <code>train_state</code> parameters, is explaining the data.
<div class="highlight"><pre><span></span><code><span class="n">loss</span><span class="p">,</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">value_and_grad</span><span class="p">(</span><span class="n">loss_fn</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
</code></pre></div>
By supplying this function to <code>jax.value_and_grad</code>, we transform it into a function that returns both the scalar value and the gradient of <code>loss_fn</code> evaluated at <code>params</code> (the <em>value</em> and <em>grad</em>).</p>
<p>Since we have defined our parameters in terms of a series of nested dictionaries, the gradient will also be a series of nested dictionaries, mirroring the parameters. Recall that, unlike the parameters, the optimizer states contain some extra, deeper nested dictionaries corresponding to the optimizer state per parameter. Take a moment, before reading the explanation, to ponder what the semantics of the following function call might be:
<div class="highlight"><pre><span></span><code><span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">adam_update</span><span class="p">,</span> <span class="n">config</span><span class="p">),</span> <span class="n">train_state</span><span class="o">.</span><span class="n">params</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">train_state</span><span class="o">.</span><span class="n">opt</span><span class="p">)</span>
</code></pre></div>
Examining the call signature of the function <code>adam_apply</code> gives us a hint:
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">adam_update</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">,</span> <span class="n">param</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">grad</span><span class="p">:</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">,</span> <span class="n">adam_state</span><span class="p">:</span> <span class="n">dot_dict</span><span class="p">):</span>
  <span class="n">adam_state</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_1</span><span class="p">)</span> <span class="o">*</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_1</span> <span class="o">*</span> <span class="n">grad</span>
  <span class="n">adam_state</span><span class="o">.</span><span class="n">nu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_2</span><span class="p">)</span> <span class="o">*</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">nu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_2</span> <span class="o">*</span> <span class="n">grad</span><span class="o">**</span><span class="mi">2</span>
  <span class="n">adam_state</span><span class="o">.</span><span class="n">count</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

  <span class="n">mu_hat</span> <span class="o">=</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">mu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_1</span> <span class="o">**</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">count</span><span class="p">[</span><span class="o">...</span><span class="p">])</span>
  <span class="n">nu_hat</span> <span class="o">=</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">nu</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">config</span><span class="o">.</span><span class="n">beta_2</span> <span class="o">**</span> <span class="n">adam_state</span><span class="o">.</span><span class="n">count</span><span class="p">[</span><span class="o">...</span><span class="p">])</span>
  <span class="n">param</span><span class="p">[</span><span class="o">...</span><span class="p">]</span> <span class="o">-=</span> <span class="n">config</span><span class="o">.</span><span class="n">learning_rate</span> <span class="o">*</span> <span class="n">mu_hat</span> <span class="o">/</span> <span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">nu_hat</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">eps_root</span><span class="p">)</span> <span class="o">+</span> <span class="n">config</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>
</code></pre></div>
Because <code>train_state.params</code> is the first argument, <code>jax.tree.map</code> uses its tree structure to guide the mapping process.<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup> This means that <code>train_state.opt</code> is traversed only as deep as the leaves of <code>train_state.params</code>. The optimizer state for each parameter is therefore passed in as a complete subtree, which allows us to easily access all relevant states (like <code>mu</code> and <code>nu</code>) for a given <code>param</code> inside <code>adam_apply</code>.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>If we wished to use different optimization algorithms and states on different parameters in our model (or freeze some parameters), we could achieve this by modifying the body of <code>adam_apply</code> and replacing <code>jax.tree.map</code> with <code>jax.tree_util.tree_map_with_path</code>, which allows the operand function to customize its behavior depending on the parameter.</p>
</div>
<h2 id="the-training-loop">The Training Loop</h2>
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">train_loop</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">):</span>
  <span class="n">record_writer</span> <span class="o">=</span> <span class="n">RecordWriter</span><span class="p">()</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">init_train_state</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">train_state</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">mutable_array</span><span class="p">,</span> <span class="n">train_state</span><span class="p">)</span>
  <span class="n">batch</span> <span class="o">=</span> <span class="nb">iter</span><span class="p">(</span><span class="n">get_dataset_on_device</span><span class="p">(</span><span class="n">config</span><span class="p">))</span>
  <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">num_train_steps</span><span class="p">):</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
    <span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</code></pre></div>
<p>During training, we have to orchestrate the flow of data between two key players: the host system and the accelerator. Ensuring smooth interplay between these systems is key to writing highly performant training code. The Python <a href="https://en.wikipedia.org/wiki/Global_interpreter_lock">GIL</a> would ordinarily pose a significant obstacle here, but to work around this, the paradigm of <a href="https://jax.readthedocs.io/en/latest/async_dispatch.html">Asynchronous Dispatch</a> adopted by JAX makes this orchestration easy to accomplish. But, in order to leverage this paradigm, we need to be mindful of how our code will be executed when structuring our training step.</p>
<h3 id="efficiency-via-asynchronous-dispatch">Efficiency via Asynchronous Dispatch</h3>
<p>One of the most important tasks performed by the host system is to fetch data and place it on the accelerators so that the accelerators are never waiting for data. The time when accelerators are waiting idle between train steps is referred to as the <em>step bubble</em>. We can leverage asynchronous dispatch to minimize the step bubble. Let's see how this works with our training loop, discarding, for the moment, the line concerning the <code>record_writer</code>.
<div class="highlight"><pre><span></span><code><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</code></pre></div>
When this code executes, Python will first query the range iterator, get <code>step</code> (with value <code>0</code>), then call <code>next(batch)</code>, which will take some time to retrieve the batch. Then, <code>train_step</code> gets called. So far, nothing out of the ordinary.</p>
<p>What happens next is interesting. Because <code>jax.jit</code>-decorated calls are non-blocking, the call to <code>train_step</code> returns to the Python interpreter immediately. While the computation is enqueued on the accelerator, no work is actually performed yet. The Python loop continues, advancing the step counter and calling <code>next(batch)</code> for the <em>next</em> iteration.</p>
<p>Once the second call to <code>train_step</code> is made, its inputs are now the mutated reference to <code>train_state</code> from the previous JIT call and a fresh batch of data. The runtime is clever and sees that in order to execute the second call to <code>train_step</code>, we first need to realize the <code>train_state</code> result of step <code>0</code> to perform the mutation. And so it fires off the computation for the first step, and, crucially, while this happens, <code>train_step</code>, once again, returns immediately, and the loop skips over again. Python now runs ahead until it encounters the <code>next(batch)</code> function at step 3, which proceeds to execute in Python, loading data, <em>while</em> the first train step is executing (for real this time). And just like that, we can simultaneously load data and perform math on the accelerator, without any traditional multiprocessing.<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup></p>
<pre class="mermaid"><code>---
displayMode: compact
---
gantt
    title Synchronous Dispatch: No Overlap
    axisFormat %

    section Host
    next(batch) 0 :gb0, 0, 1000s
    next(batch) 1 :gb1, after ajc0, 1000s
    next(batch) 2 :gb2, after ajc1, 1000s

    section Accelerator
    train_step 0 :ajc0, after gb0, 2000s
    train_step 1 :ajc1, after gb1, 2000s</code></pre>
<pre class="mermaid"><code>---
displayMode: compact
---
gantt
    title JAX Asynchronous Dispatch: Host-Device Overlap
    axisFormat %

    section Host
    %% Task: id, name, start, duration_or_end
    next(batch) 0 :gb0, 0, 1000s
    next(batch) 1 :gb1, after gb0, 1000s
    next(batch) 2 :gb2, after gb1, 1000s
    next(batch) 3 :gb3, after jc0, 1000s
    next(batch) 4 :gb4, after jc1, 1000s

    section Accelerator
    %% Task: id, name, start, duration_or_end
    train_step 0 :jc0, after gb1, 2000s
    train_step 1 :jc1, after jc0, 2000s
    train_step 2 :jc2, after jc1, 2000s</code></pre>
<h3 id="common-mistakes">Common Mistakes</h3>
<p>When writing asynchronous dispatch code in Python, there are two primary mistakes one should be wary of so as not to interrupt our careful orchestration of compute.</p>
<h4 id="requesting-device-to-host-transfers">Requesting device-to-host transfers</h4>
<p>Up until now, we have ignored what happens to the variable <code>metrics</code>. Indeed, if this is left dangling, nothing will happen, and we will achieve good overlap just as advertised. However, more often than not, we would like to observe telemetry from our train step, such as the current loss, gradient statistics, and so on. Suppose we were to insert code such as:
<div class="highlight"><pre><span></span><code><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="nb">print</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</code></pre></div>
Instead of the loop ticking over, <code>print</code> will incur a device-to-host transfer of whatever on-device arrays are in <code>metrics</code>. This interrupts the Python interpreter, and the code is forced to execute synchronously, producing a step bubble. The solution is slightly counterintuitive: at each step, we gather the telemetry for the <em>previous</em> step.
<div class="highlight"><pre><span></span><code><span class="k">class</span><span class="w"> </span><span class="nc">RecordWriter</span><span class="p">:</span>
  <span class="n">prev_metrics</span> <span class="o">=</span> <span class="kc">None</span>

  <span class="k">def</span><span class="w"> </span><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">cur_metrics</span><span class="p">:</span> <span class="nb">dict</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">prev_metrics</span><span class="p">,</span> <span class="n">log_metrics</span> <span class="o">=</span> <span class="n">cur_metrics</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">prev_metrics</span>
    <span class="k">if</span> <span class="n">log_metrics</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
      <span class="k">return</span>
    <span class="nb">print</span><span class="p">(</span><span class="o">*</span><span class="n">it</span><span class="o">.</span><span class="n">starmap</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{}</span><span class="s2">: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">,</span> <span class="n">log_metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()),</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot;</span><span class="se">\t</span><span class="s2">&quot;</span><span class="p">)</span>
</code></pre></div>
and
<div class="highlight"><pre><span></span><code><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
</code></pre></div>
A small helper function like this is essential to achieve good overlap and make the most of the resources of our host system and our accelerator. Of course, the simple <code>print</code> statement here can be swapped out for any Python operation that requests data from the accelerator.</p>
<h4 id="interrupting-the-accelerator">Interrupting the accelerator</h4>
<p>The other common way in which we can waste spectacular amounts of cloud compute money is by unintentionally enqueuing math operations on the accelerator outside of the train step. Suppose we are using a cosine learning rate schedule.
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">learning_rate</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">init_value</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10_000</span><span class="p">,</span> <span class="n">alpha</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">):</span>
    <span class="n">cosine_decay</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">pi</span> <span class="o">*</span> <span class="n">jnp</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">count</span><span class="p">,</span> <span class="n">decay_steps</span><span class="p">)</span> <span class="o">/</span> <span class="n">decay_steps</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">init_value</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">cosine_decay</span>
</code></pre></div>
A common pattern is to want to visualize the schedule alongside the other metrics we're gathering. However, even if we use the clever <code>record_writer</code> class we defined earlier, the following code will create a bubble on the accelerator.
<div class="highlight"><pre><span></span><code><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">(</span><span class="n">step</span><span class="p">)}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</code></pre></div>
This is because we have used <code>jax.numpy</code> in our calculations. When <code>jnp.minimum</code> is called, the Python integer <code>step</code> is promoted to a <code>jax.Array</code> and transferred to the accelerator (a host-to-device transfer). The calculation is now enqueued on the accelerator, outside our main <code>train_step</code>. To <code>print</code> the result, the value must be transferred back to the host (a device-to-host transfer). This round-trip forces the accelerator to synchronize with the host, and we have thrown away money by creating a performance bubble.</p>
<p>The two ways to avoid this are to use NumPy for these calculations or to use the <code>jax.default_device</code> context manager.
<div class="highlight"><pre><span></span><code><span class="n">metrics</span> <span class="o">=</span> <span class="n">train_step</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train_state</span><span class="p">,</span> <span class="nb">next</span><span class="p">(</span><span class="n">batch</span><span class="p">))</span>
<span class="k">with</span> <span class="n">jax</span><span class="o">.</span><span class="n">default_device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">):</span>
  <span class="n">record_writer</span><span class="p">({</span><span class="s2">&quot;step&quot;</span><span class="p">:</span> <span class="n">step</span><span class="p">,</span> <span class="s2">&quot;learning_rate&quot;</span><span class="p">:</span> <span class="n">learning_rate</span><span class="p">(</span><span class="n">step</span><span class="p">)}</span> <span class="o">|</span> <span class="n">metrics</span><span class="p">)</span>
</code></pre></div></p>
<h3 id="data-loading">Data Loading</h3>
<p>In addition to overlapping the actual loading of the data (that is, retrieving it from network storage to the host), JAX also allows us to overlap the host-to-device transfer of the data itself with the computation of the train step. The special function <code>jax.device_put</code> is carefully designed to be non-blocking, executing asynchronously, which makes it perfectly fine to use in the context of our train step. However, there is a more convenient function specifically designed for the task of loading data.</p>
<p>In the following code, <code>dataset</code> is an ordinary Python iterator that yields a <code>dict</code> of batched data. By mapping over this iterator with <code>jax.make_array_from_process_local_data</code>, we generate a new iterator. Yielding from this new iterator will generate data placed on the device, ready for consumption by our train step. Internally, it will <code>jax.tree.map</code> to create <code>jax.Array</code> objects and queue them to be transferred to the device. Provided the data can be batched fast enough, on both TPUs and GPUs, these transfers will be overlapped with the train step computation.
<div class="highlight"><pre><span></span><code><span class="k">def</span><span class="w"> </span><span class="nf">get_dataset_on_device</span><span class="p">(</span><span class="n">config</span><span class="p">:</span> <span class="n">Config</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Iterator</span><span class="p">[</span><span class="nb">dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">jax</span><span class="o">.</span><span class="n">Array</span><span class="p">]]:</span>
  <span class="n">datset</span> <span class="o">=</span> <span class="n">get_dataset</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
  <span class="n">sharding</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">P</span><span class="p">(</span><span class="n">config</span><span class="o">.</span><span class="n">mesh_axis_names</span><span class="p">)</span>
  <span class="k">return</span> <span class="nb">map</span><span class="p">(</span><span class="n">ft</span><span class="o">.</span><span class="n">partial</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">make_array_from_process_local_data</span><span class="p">,</span> <span class="n">sharding</span><span class="p">),</span> <span class="n">datset</span><span class="p">)</span>
</code></pre></div></p>
<h2 id="achieving-high-performance">Achieving High Performance</h2>
<p>In this section, we will describe the three primary forms of model parallelism that are useful for training. During training, <em>throughput</em> is of paramount importance; that is, we wish to maximize the average number of operations per second. This contrasts with inference, where the goal is to minimize <em>latency</em> by ensuring all the operations happen in as little time as possible. Keeping throughput in mind as our ultimate goal for training, this section introduces the three primary strategies for sharding during training. For each strategy, we outline the JAX shardings that implement it and describe the collectives involved so that when studying program traces, you'll have landmarks to look for to confirm that the program is behaving as expected. The sharding variables we define in the code blocks below correspond to their uses in the <a href="#train-state-initialization">initialization</a> and <a href="#model-forward-pass">model forward pass</a>.</p>
<h3 id="data-parallel">Data Parallel</h3>
<p>Data parallel is the most common and easy-to-understand form of parallelism. In this scheme, each accelerator stores a complete copy of the model parameters, and we shard activations along the batch axis to split the computation of the gradients. To compute the gradients, each accelerator performs an individual forward and backward pass. Then, before the parameters are updated, XLA inserts an <a href="https://openxla.org/xla/operation_semantics#allreduce"><code>AllReduce</code></a> to share the updates and keep the models in sync.</p>
<p><div class="highlight"><span class="filename">Mesh</span><pre><span></span><code><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(),</span> <span class="p">(</span><span class="s1">&#39;devices&#39;</span><span class="p">,))</span>
</code></pre></div>
<div class="highlight"><span class="filename">Parameter Shardings</span><pre><span></span><code><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_qkv</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_out</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_in</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_kernel</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_bias</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out_kernel</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">out_bias</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><span class="filename">Activation Shardings</span><pre><span></span><code><span class="n">act_ids</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">)</span>
<span class="n">act_seq</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_att</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_hidden</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="err">``</span>

<span class="c1">### Fully-Sharded Data Parallel (FSDP)</span>
<span class="n">The</span> <span class="n">drawback</span> <span class="n">of</span> <span class="n">data</span><span class="o">-</span><span class="n">parallel</span> <span class="n">sharding</span> <span class="ow">is</span> <span class="n">that</span> <span class="n">we</span> <span class="n">have</span> <span class="n">to</span> <span class="n">keep</span> <span class="n">multiple</span><span class="p">,</span> <span class="n">full</span><span class="p">,</span> <span class="n">redundant</span> <span class="n">copies</span> <span class="n">of</span> <span class="n">the</span> <span class="n">model</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">HBM</span><span class="o">.</span> <span class="n">This</span> <span class="ow">is</span> <span class="n">a</span> <span class="n">very</span> <span class="n">performant</span> <span class="n">strategy</span> <span class="k">for</span> <span class="n">small</span> <span class="n">models</span><span class="p">,</span> <span class="n">but</span> <span class="n">since</span> <span class="n">HBM</span> <span class="ow">is</span> <span class="ow">in</span> <span class="n">short</span> <span class="n">supply</span><span class="p">,</span> <span class="n">we</span> <span class="n">need</span> <span class="n">to</span> <span class="n">shard</span> <span class="n">the</span> <span class="n">model</span> <span class="n">parameters</span> <span class="k">as</span> <span class="n">well</span><span class="o">.</span> <span class="n">In</span> <span class="n">the</span> <span class="o">*</span><span class="n">Fully</span><span class="o">-</span><span class="n">Sharded</span> <span class="n">Data</span> <span class="n">Parallel</span> <span class="p">(</span><span class="n">FSDP</span><span class="p">)</span><span class="o">*</span> <span class="n">strategy</span><span class="p">,</span> <span class="n">we</span> <span class="n">shard</span> <span class="n">both</span> <span class="n">the</span> <span class="n">model</span> <span class="ow">and</span> <span class="n">the</span> <span class="n">parameters</span><span class="o">.</span>

<span class="n">Now</span><span class="p">,</span> <span class="k">as</span> <span class="n">the</span> <span class="n">forward</span> <span class="k">pass</span> <span class="n">happens</span><span class="p">,</span> <span class="n">the</span> <span class="n">parameters</span> <span class="n">are</span><span class="p">,</span> <span class="n">one</span><span class="o">-</span><span class="n">by</span><span class="o">-</span><span class="n">one</span><span class="p">,</span> <span class="n">unsharded</span> <span class="p">(</span><span class="n">via</span> <span class="p">[</span><span class="err">`</span><span class="n">AllGather</span><span class="err">`</span><span class="p">](</span><span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">openxla</span><span class="o">.</span><span class="n">org</span><span class="o">/</span><span class="n">xla</span><span class="o">/</span><span class="n">operation_semantics</span><span class="c1">#allgather)) into whole arrays before they are applied to the activations. This unsharding is brief and temporary, however, leading to a large saving in HBM. In the backward pass, each `AllGather` becomes a `ReduceScatter`. Then there is a final `ReduceScatter` at the optimizer update to synchronize gradients. Compared with Data parallelism, the total communication traffic is 50% highter, but we our HBM pressure is reduced by the size of the model divided by the number of devices.  </span>

<span class="err">```</span><span class="n">python</span> <span class="p">{</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Mesh&quot;</span><span class="p">}</span>
<span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">(),</span> <span class="p">(</span><span class="s1">&#39;fsdp&#39;</span><span class="p">,))</span>
</code></pre></div>
<div class="highlight"><span class="filename">Parameter Shardings</span><pre><span></span><code><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_qkv</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_out</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_in</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">in_kernel</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_bias</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out_kernel</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">out_bias</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><span class="filename">Activation Shardings</span><pre><span></span><code><span class="n">act_ids</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">act_seq</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_att</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_hidden</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
</code></pre></div></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While FSDP entails a great deal more communication than data parallel, in practice we are able to overlap the communication with the compute, thereby hiding it and achieving the same throughput at a drastically improved HBM budget.</p>
<p>TODO: Mention specific flags to enable this for TPUs vs GPUs. On TPUs it's the async all-gather and reduce-scatter functionality. On GPUs, you have to set threshold values as specified in the NVIDIA JAX-Toolbox repository.</p>
</div>
<h3 id="tensor-parallel">Tensor Parallel</h3>
<p>If our model is large enough and structured appropriately, it becomes beneficial to partition the computation within a single example across our accelerators. Using a matrix multiplication as an example, we can spread the large matrix multiplications over two or four accelerators. This entails significantly more communication, and so this strategy only works for computations with a very high arithmetic intensity, such as extremely large matrix multiplications.</p>
<p>With multi-head self-attention, we opt to shard along the heads with a replicated sequence axis, since this offers the most natural amount of parallelism. If the MLP is large enough we can also efficiently shard the matrix multiplications.</p>
<p><div class="highlight"><span class="filename">Mesh</span><pre><span></span><code><span class="n">mesh</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">sharding</span><span class="o">.</span><span class="n">Mesh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">jax</span><span class="o">.</span><span class="n">devices</span><span class="p">())</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">4</span><span class="p">),</span> <span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">))</span>
</code></pre></div>
<div class="highlight"><span class="filename">Parameter Shardings</span><pre><span></span><code><span class="n">pos_embed</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
<span class="n">att_qkv</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">att_out</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">mlp_in</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
<span class="n">mlp_out</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">in_kernel</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">in_bias</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
<span class="n">out_kernel</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">out_bias</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="kc">None</span><span class="p">)</span>
</code></pre></div>
<div class="highlight"><span class="filename">Activation Shardings</span><pre><span></span><code><span class="n">act_ids</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">)</span>
<span class="n">act_seq</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_att</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">act_hidden</span> <span class="o">=</span> <span class="n">PartitionSpec</span><span class="p">(</span><span class="s2">&quot;fsdp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="s2">&quot;tensor&quot;</span><span class="p">)</span>
</code></pre></div></p>
<hr />
<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Of course, all dictionaries are order-preserving in modern Python, so this is somewhat redundant.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>This is accomplished by using the <code>zeros_like</code> constructor, but we could have specified the sharding manually using the <code>devices</code> argument of many of the <code>jax.numpy</code> functions.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>We could have achieved the same behavior equivalently by ordering <code>grad</code> first.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>For the purposes of this explanation, you can think of <code>next(batch)</code> as just a sleep.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
</ol>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.tabs", "content.footnote.tooltips", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../assets/javascripts/workers/search.d50fe291.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.56ea9cef.min.js"></script>
      
        <script src="../javascripts/mathjax.js"></script>
      
        <script src="https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>